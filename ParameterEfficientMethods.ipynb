{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4CFZeGzHC_Kh",
        "nixR57TmDEO1",
        "iZun5caUDMtX",
        "UFgBDrY4aP44"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models"
      ],
      "metadata": {
        "id": "ub1JfyLY2ibV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -q transformers\n",
        "!pip install -U -q adapter-transformers==3.0.1 datasets==2.3.2 tokenizers==0.12.1 huggingface-hub==0.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNlVEaftLrlY",
        "outputId": "43e0bce2-08a3-4945-a5bc-c8a8cf1090d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed (Y/n)? Y\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.3/362.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/pedb\n",
        "#!gdown 1JSlm8MYDbNjpMPnKbb91T-xZnlWAZmZl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsR9buwxXGBS",
        "outputId": "03a5fd86-d2e6-4f32-a49d-acbc72eb1622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1J2HTHrXOwKKcavqhnbvNDxRQ43-djhUQ/pedb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crows-Pairs"
      ],
      "metadata": {
        "id": "4CFZeGzHC_Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"none\" \\\n",
        "--task_type \"masked_lm\" --dataset_name \"crows\" --bias_type \"gender\" --seed 42 --output_dir ''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La4uaDpxb02I",
        "outputId": "5c288bee-cede-4771-9ab0-2b6aedeb6bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-05 19:23:34.739442: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-05 19:23:34.739512: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-05 19:23:34.739552: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-05 19:23:34.748605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-05 19:23:36.008053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 570/570 [00:00<00:00, 3.08MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 143kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 1.39MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 2.85MB/s]\n",
            "Downloading: 100% 420M/420M [00:07<00:00, 56.6MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tunable_param is 109514298, frozen_param is 0\n",
            "Evaluating gender examples.\n",
            "100% 262/262 [02:41<00:00,  1.62it/s]\n",
            "====================================================================================================\n",
            "Total examples: 262\n",
            "Metric score: 57.25\n",
            "Stereotype score: 57.86\n",
            "Anti-stereotype score: 56.31\n",
            "Num. neutral: 0.0\n",
            "====================================================================================================\n",
            "\n",
            "Metric: 57.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"SelfDebiasBertForMaskedLM\" \\\n",
        "--task_type \"masked_lm\" --dataset_name \"crows\" --bias_type \"gender\" --seed 42 --output_dir ''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGk4t3wZW8Lr",
        "outputId": "b079bc7e-c069-4c71-f68d-b8b86099022a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-05 19:35:31.805180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-05 19:35:31.805270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-05 19:35:31.805346: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-05 19:35:31.823222: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-05 19:35:33.275192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating gender examples.\n",
            "100% 262/262 [06:33<00:00,  1.50s/it]\n",
            "====================================================================================================\n",
            "Total examples: 262\n",
            "Metric score: 53.05\n",
            "Stereotype score: 50.31\n",
            "Anti-stereotype score: 57.28\n",
            "Num. neutral: 0.0\n",
            "====================================================================================================\n",
            "\n",
            "Metric: 53.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "#%cd experiments\n",
        "!python sentence_debias_subspace.py --model \"BertModel\" --model_name_or_path \"bert-base-uncased\" --bias_type \"gender\"\n",
        "#!python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"SentenceDebiasBertForMaskedLM\" --task_type \\\n",
        "#\"masked_lm\" --dataset_name \"crows\" --bias_type \"gender\" --seed 42 --output_dir ''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHpj1kBjDi_d",
        "outputId": "0239c4a0-1334-4c06-c50d-6eccc1795944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-06 04:03:43.095243: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 04:03:43.095300: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 04:03:43.095335: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 04:03:43.102826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-06 04:03:44.115574: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Computing bias subspace:\n",
            " - persistent_dir: /content/drive/MyDrive/pedb\n",
            " - model_name_or_path: bert-base-uncased\n",
            " - model: BertModel\n",
            " - bias_type: gender\n",
            " - batch_size: 32\n",
            "Downloading: 100% 570/570 [00:00<00:00, 3.43MB/s]\n",
            "Downloading: 100% 420M/420M [00:05<00:00, 81.4MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 148kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 3.10MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 6.66MB/s]\n",
            "Encoding gender examples:   7% 2414/34247 [16:40<3:39:50,  2.41it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/pedb/experiments/sentence_debias_subspace.py\", line 98, in <module>\n",
            "    bias_direction = compute_gender_subspace(\n",
            "  File \"/content/drive/MyDrive/pedb/experiments/../bias_bench/debias/sentence_debias.py\", line 69, in compute_gender_subspace\n",
            "    all_embeddings_male.append(embedding_male.cpu().numpy())\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stereoset"
      ],
      "metadata": {
        "id": "nixR57TmDEO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"none\" \\\n",
        "--task_type \"masked_lm\" --dataset_name \"stereoset\" --bias_type \"gender\" --seed 42 --output_dir ''"
      ],
      "metadata": {
        "id": "MeVyv0LBkvHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"SelfDebiasBertForMaskedLM\" \\\n",
        "--task_type \"masked_lm\" --dataset_name \"stereoset\" --bias_type \"gender\" --seed 42 --output_dir ''"
      ],
      "metadata": {
        "id": "eaSh-1-bnToV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python stereoset_evaluation.py --save_dir \"checkpoints/bert-base-uncased\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fg51jqPKm2hT",
        "outputId": "45baa0ca-29e2-4ef7-ec31-887e76cfb5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 2313.0\n",
            "\t\tLM Score: 85.73970402503635\n",
            "\t\tSS Score: 60.278684896276104\n",
            "\t\tICAT Score: 68.11387600956986\n",
            "\tprofession\n",
            "\t\tCount: 7194.0\n",
            "\t\tLM Score: 83.85202686230224\n",
            "\t\tSS Score: 58.93389453546499\n",
            "\t\tICAT Score: 68.86952357084654\n",
            "\trace\n",
            "\t\tCount: 8928.0\n",
            "\t\tLM Score: 84.00827460718995\n",
            "\t\tSS Score: 57.02967880452161\n",
            "\t\tICAT Score: 72.19725085897807\n",
            "\treligion\n",
            "\t\tCount: 741.0\n",
            "\t\tLM Score: 84.21150995925436\n",
            "\t\tSS Score: 59.70419362150916\n",
            "\t\tICAT Score: 67.86741400316933\n",
            "\toverall\n",
            "\t\tCount: 6392.0\n",
            "\t\tLM Score: 84.17236429175225\n",
            "\t\tSS Score: 58.24009298588702\n",
            "\t\tICAT Score: 70.30060211963236\n",
            "overall\n",
            "\tCount: 6392.0\n",
            "\tLM Score: 84.17236429175225\n",
            "\tSS Score: 58.24009298588702\n",
            "\tICAT Score: 70.30060211963236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python stereoset_evaluation.py --save_dir \"results/SelfDebiasBertForMaskedLM_bert-base-uncased\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuquWpiYnc9K",
        "outputId": "9f094b13-f919-4e5d-86cf-1413add09a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "intrasentence\n",
            "\tgender\n",
            "\t\tCount: 2313.0\n",
            "\t\tLM Score: 86.07256899263395\n",
            "\t\tSS Score: 59.08411439653319\n",
            "\t\tICAT Score: 70.4347077299823\n",
            "\tprofession\n",
            "\t\tCount: 7194.0\n",
            "\t\tLM Score: 83.47993556846767\n",
            "\t\tSS Score: 58.278976355049345\n",
            "\t\tICAT Score: 69.65736731461993\n",
            "\trace\n",
            "\t\tCount: 8928.0\n",
            "\t\tLM Score: 83.56631606994864\n",
            "\t\tSS Score: 55.11904918199566\n",
            "\t\tICAT Score: 75.0107144315434\n",
            "\treligion\n",
            "\t\tCount: 741.0\n",
            "\t\tLM Score: 83.83388579130762\n",
            "\t\tSS Score: 59.47567529263241\n",
            "\t\tICAT Score: 67.94623218574641\n",
            "\toverall\n",
            "\t\tCount: 6392.0\n",
            "\t\tLM Score: 83.85483454118624\n",
            "\t\tSS Score: 56.947789004337345\n",
            "\t\tICAT Score: 72.20272059347062\n",
            "overall\n",
            "\tCount: 6392.0\n",
            "\tLM Score: 83.85483454118624\n",
            "\tSS Score: 56.947789004337345\n",
            "\tICAT Score: 72.20272059347062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wikitext"
      ],
      "metadata": {
        "id": "iZun5caUDMtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"gpt2\" --prompt_model \"none\" --task_type \"causal_lm\" \\\n",
        "--dataset_name \"wikitext2\" --max_seq_length 992 --bias_type \"gender\" --seed 42 --output_dir '' --per_device_train_batch_size 8"
      ],
      "metadata": {
        "id": "ErBjEkYCgntf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"gpt2\" --prompt_model \"SelfDebiasGPT2LMHeadModel\" --task_type \"causal_lm\" \\\n",
        "--dataset_name \"wikitext2\" --max_seq_length 992 --bias_type \"gender\" --seed 42 --output_dir '' --per_device_train_batch_size 8"
      ],
      "metadata": {
        "id": "hLXzQTMVhCyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"none\" --task_type \"masked_lm\" \\\n",
        "--dataset_name \"wikitext2\" --max_seq_length 480 --bias_type \"gender\" --seed 42 --output_dir '' --per_device_train_batch_size 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugTdnlG_Y9WW",
        "outputId": "15bc88c5-b83e-464a-f057-c8cae1147e1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-06 15:38:35.746804: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-06 15:38:35.746869: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-06 15:38:35.746909: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-06 15:38:35.757864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-06 15:38:37.636937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 570/570 [00:00<00:00, 2.74MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 148kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 1.17MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 793kB/s]\n",
            "Downloading: 100% 420M/420M [00:23<00:00, 18.6MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "tunable_param is 109514298, frozen_param is 0\n",
            "Downloading builder script: 8.48kB [00:00, 15.5MB/s]       \n",
            "Downloading metadata: 6.84kB [00:00, 15.7MB/s]       \n",
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n",
            "Downloading data: 100% 4.72M/4.72M [00:01<00:00, 3.25MB/s]\n",
            "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n",
            "Parameter 'function'=<function get_tokenized_datasets.<locals>.tokenize_function at 0x7e3cbf51bd90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on every text in dataset:  60% 3/5 [00:00<00:00,  4.49ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on every text in dataset: 100% 5/5 [00:00<00:00,  5.48ba/s]\n",
            "Grouping texts in chunks of 480: 100% 5/5 [00:02<00:00,  2.12ba/s]\n",
            "preparing examples...\n",
            "100% 585/585 [00:10<00:00, 54.75it/s]\n",
            "evaluating...\n",
            "100% 8505/8505 [3:07:43<00:00,  1.32s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python evaluate.py --model_name_or_path \"bert-base-uncased\" --prompt_model \"SelfDebiasBertForMaskedLM\" \\\n",
        "--task_type \"masked_lm\" --dataset_name \"wikitext2\" --max_seq_length 480 --bias_type \"gender\" --seed 42 --output_dir '' --per_device_train_batch_size 32"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk5pegIMZ463",
        "outputId": "29817596-ea3e-40e9-c61d-0c6c7ca480fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-07 01:15:26.773736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 01:15:26.773794: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 01:15:26.773829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 01:15:26.781754: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 01:15:27.793887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading: 100% 570/570 [00:00<00:00, 3.09MB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 168kB/s]\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 4.85MB/s]\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 6.19MB/s]\n",
            "Downloading: 100% 420M/420M [00:07<00:00, 56.5MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading builder script: 8.48kB [00:00, 17.8MB/s]       \n",
            "Downloading metadata: 6.84kB [00:00, 16.1MB/s]       \n",
            "Downloading and preparing dataset wikitext/wikitext-2-raw-v1 (download: 4.50 MiB, generated: 12.90 MiB, post-processed: Unknown size, total: 17.40 MiB) to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n",
            "Downloading data: 100% 4.72M/4.72M [00:00<00:00, 20.9MB/s]\n",
            "Dataset wikitext downloaded and prepared to /root/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n",
            "Parameter 'function'=<function get_tokenized_datasets.<locals>.tokenize_function at 0x7bb6cf8afd90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on every text in dataset:  60% 3/5 [00:01<00:00,  2.72ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n",
            "Running tokenizer on every text in dataset: 100% 5/5 [00:01<00:00,  3.62ba/s]\n",
            "Grouping texts in chunks of 480: 100% 5/5 [00:02<00:00,  2.09ba/s]\n",
            "preparing examples...\n",
            "100% 585/585 [00:09<00:00, 62.10it/s]\n",
            "evaluating...\n",
            " 48% 4062/8505 [3:16:41<3:35:08,  2.91s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/pedb/evaluate.py\", line 185, in <module>\n",
            "    ) if model_args.task_type=='causal_lm' else model.compute_loss_self_debiasing(\n",
            "  File \"/content/drive/MyDrive/pedb/bias_bench/debias/self_debias/modeling.py\", line 278, in compute_loss_self_debiasing\n",
            "    token_logits = self.get_token_logits_self_debiasing(\n",
            "  File \"/content/drive/MyDrive/pedb/bias_bench/debias/self_debias/modeling.py\", line 163, in get_token_logits_self_debiasing\n",
            "    if torch.any(mask_positions[:, idx]):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('results/SelfDebiasGPT2LMHeadModel_gpt2/results/wikitext2.json', 'r') as json_file:\n",
        "    gpt2_selfbias = json.load(json_file)\n",
        "with open('checkpoints/gpt2/results/wikitext2.json', 'r') as json_file:\n",
        "    gpt2 = json.load(json_file)\n",
        "#with open('results/SelfDebiasBertForMaskedLM_bert-base-uncased/results/wikitext2.json', 'r') as json_file:\n",
        "#    bert_selfbias = json.load(json_file)\n",
        "with open('checkpoints/bert-base-uncased/results/wikitext2.json', 'r') as json_file:\n",
        "    bert = json.load(json_file)\n",
        "\n",
        "print(\"GPT2 wikitext2: \",gpt2)\n",
        "print(\"GPT2 wikitext2 with Selfbias: \",gpt2_selfbias)\n",
        "\n",
        "print(\"BERT wikitext2: \",bert)\n",
        "#print(\"BERT wikitext2 with Selfbias: \",bert_selfbias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Efps7QTdYLqr",
        "outputId": "7a077908-aa8f-47a6-aee5-b1c13ac1957d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2 wikitext2:  {'perplexity_992': 29.668254852294922}\n",
            "GPT2 wikitext2 with Selfbias:  {'perplexity_992': 31.482067108154297}\n",
            "BERT wikitext2:  {'perplexity_480': 5.1668381690979}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "UFgBDrY4aP44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python -u debias_adapter.py \\\n",
        "\t--model_name_or_path \"gpt2\" \\\n",
        "\t--task_type \"causal_lm\" \\\n",
        "\t--train_file \"data/wikipedia-10.txt\" \\\n",
        "\t--max_seq_length 1024 \\\n",
        "\t--line_by_line \\\n",
        "\t--bias_type \"gender\" \\\n",
        "\t--cda_mode \"partial\" \\\n",
        "\t--output_dir \"checkpoints/gpt2-adapter-rf48\" \\\n",
        "\t--do_train \\\n",
        "\t--per_device_train_batch_size 4 \\\n",
        "\t--gradient_accumulation_steps 2 \\\n",
        "\t--learning_rate 5e-4 \\\n",
        "\t--num_train_epochs 2 \\\n",
        "\t--save_strategy \"no\" \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--seed 42 \\\n",
        "\t--down_sample 0.2 \\\n",
        "\t--adapter_config \"pfeiffer\" \\\n",
        "\t--adapter_reduction_factor 48 \\\n",
        "\t#> run_gpt2_adapter_rf48.out 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoEUGEozXXyS",
        "outputId": "5e6835da-1f59-476a-a6b9-69a098a8896a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-05 20:43:22.152028: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-05 20:43:22.152087: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-05 20:43:22.152125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-05 20:43:22.159861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-05 20:43:23.133582: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "12/05/2023 20:43:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/05/2023 20:43:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/gpt2-adapter-rf48/runs/Dec05_20-43-26_f62bee007c7a,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=checkpoints/gpt2-adapter-rf48,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/gpt2-adapter-rf48,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:648] 2023-12-05 20:43:26,567 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-05 20:43:26,569 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2023-12-05 20:43:26,815 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2023-12-05 20:43:27,052 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-05 20:43:27,053 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-05 20:43:28,703 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2023-12-05 20:43:28,946 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-05 20:43:28,946 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2023-12-05 20:43:29,259 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1702] 2023-12-05 20:43:35,060 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1710] 2023-12-05 20:43:35,060 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|configuration.py:617] 2023-12-05 20:43:35,087 >> Adding adapter 'causal_lm'.\n",
            "tunable_param is 304320, frozen_param is 124439808\n",
            "12/05/2023 20:43:35 - WARNING - datasets.builder - Using custom data configuration default-855e5f9a96ab01c1\n",
            "12/05/2023 20:43:35 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/05/2023 20:43:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "12/05/2023 20:43:35 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n",
            "12/05/2023 20:43:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "100% 1/1 [00:00<00:00, 83.01it/s]\n",
            "12/05/2023 20:43:35 - WARNING - datasets.builder - Using custom data configuration default-855e5f9a96ab01c1\n",
            "12/05/2023 20:43:35 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/05/2023 20:43:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "12/05/2023 20:43:35 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n",
            "12/05/2023 20:43:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "12/05/2023 20:43:36 - WARNING - datasets.builder - Using custom data configuration default-855e5f9a96ab01c1\n",
            "12/05/2023 20:43:36 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/05/2023 20:43:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "12/05/2023 20:43:36 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n",
            "12/05/2023 20:43:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08\n",
            "12/05/2023 20:43:36 - WARNING - datasets.fingerprint - Parameter 'function'=<function get_tokenized_datasets.<locals>.tokenize_function at 0x79471c132f80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/5350 [00:00<?, ?ba/s]12/05/2023 20:43:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 5350/5350 [17:58<00:00,  4.96ba/s]\n",
            "12/05/2023 21:01:34 - INFO - datasets.fingerprint - Parameter 'function'=<function get_tokenized_datasets.<locals>.tokenize_function at 0x79471c132f80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset line_by_line:   0% 0/282 [00:00<?, ?ba/s]12/05/2023 21:01:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset line_by_line: 100% 282/282 [01:06<00:00,  4.25ba/s]\n",
            "12/05/2023 21:02:41 - INFO - datasets.fingerprint - Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x79471c133010> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Applying counterfactual augmentation:   0% 0/5330 [00:00<?, ?ba/s]12/05/2023 21:02:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-3eb13b9046685257.arrow\n",
            "Applying counterfactual augmentation: 100% 5330/5330 [2:39:31<00:00,  1.80s/ba]\n",
            "12/05/2023 23:42:13 - INFO - datasets.fingerprint - Parameter 'function'=<function Dataset.map.<locals>.decorate.<locals>.decorated at 0x79472207e440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Applying counterfactual augmentation:   0% 0/282 [00:00<?, ?ba/s]12/05/2023 23:42:15 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-855e5f9a96ab01c1/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-23b8c1e9392456de.arrow\n",
            "Applying counterfactual augmentation: 100% 282/282 [09:56<00:00,  2.11s/ba]\n",
            "12/05/2023 23:52:17 - INFO - datasets.arrow_dataset - Dataset saved in data/wikipedia-10-gender-1024-linebyline/train\n",
            "12/05/2023 23:52:17 - INFO - datasets.arrow_dataset - Dataset saved in data/wikipedia-10-gender-1024-linebyline/validation\n",
            "sampled train_dataset size: 468102 (20.0% of the augmented dataset)\n",
            "[INFO|trainer.py:229] 2023-12-05 23:52:28,014 >> The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: special_tokens_mask.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1254] 2023-12-05 23:52:28,933 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2023-12-05 23:52:28,933 >>   Num examples = 468102\n",
            "[INFO|trainer.py:1256] 2023-12-05 23:52:28,933 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1257] 2023-12-05 23:52:28,933 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1258] 2023-12-05 23:52:28,933 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1259] 2023-12-05 23:52:28,933 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1260] 2023-12-05 23:52:28,933 >>   Total optimization steps = 117026\n",
            "{'loss': 3.7255, 'learning_rate': 0.0004978637225915609, 'epoch': 0.01}\n",
            "{'loss': 3.6693, 'learning_rate': 0.0004957274451831217, 'epoch': 0.02}\n",
            "{'loss': 3.6579, 'learning_rate': 0.0004935911677746825, 'epoch': 0.03}\n",
            "{'loss': 3.6662, 'learning_rate': 0.0004914548903662434, 'epoch': 0.03}\n",
            "{'loss': 3.6548, 'learning_rate': 0.0004893186129578043, 'epoch': 0.04}\n",
            "{'loss': 3.6387, 'learning_rate': 0.0004871823355493651, 'epoch': 0.05}\n",
            "{'loss': 3.6233, 'learning_rate': 0.00048504605814092593, 'epoch': 0.06}\n",
            "{'loss': 3.6279, 'learning_rate': 0.0004829097807324868, 'epoch': 0.07}\n",
            "{'loss': 3.6293, 'learning_rate': 0.0004807735033240477, 'epoch': 0.08}\n",
            "{'loss': 3.6379, 'learning_rate': 0.0004786372259156085, 'epoch': 0.09}\n",
            "{'loss': 3.641, 'learning_rate': 0.0004765009485071693, 'epoch': 0.09}\n",
            "{'loss': 3.6307, 'learning_rate': 0.0004743646710987302, 'epoch': 0.1}\n",
            "{'loss': 3.6408, 'learning_rate': 0.0004722283936902911, 'epoch': 0.11}\n",
            "{'loss': 3.6348, 'learning_rate': 0.0004700921162818519, 'epoch': 0.12}\n",
            "{'loss': 3.6175, 'learning_rate': 0.0004679558388734128, 'epoch': 0.13}\n",
            "  7% 8000/117026 [40:25<9:40:34,  3.13it/s]Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/pedb/debias_adapter.py\", line 241, in <module>\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint) # TrainOutput object\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1450, in train\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1573, in _maybe_log_save_evaluate\n",
            "    self.log(logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1886, in log\n",
            "    self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 381, in on_log\n",
            "    return self.call_event(\"on_log\", args, state, control, logs=logs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 388, in call_event\n",
            "    result = getattr(callback, event)(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations.py\", line 526, in on_log\n",
            "    self.tb_writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 1233, in flush\n",
            "    writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/writer.py\", line 146, in flush\n",
            "    self.event_writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 125, in flush\n",
            "    self._async_writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/event_file_writer.py\", line 190, in flush\n",
            "    self._writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorboard/summary/writer/record_writer.py\", line 43, in flush\n",
            "    self._writer.flush()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\", line 221, in flush\n",
            "    self._writable_file.flush()\n",
            "tensorflow.python.framework.errors_impl.FailedPreconditionError: checkpoints/gpt2-adapter-rf48/runs/Dec05_20-43-26_f62bee007c7a/events.out.tfevents.1701820348.f62bee007c7a.6407.0; Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python -u debias_adapter.py \\\n",
        "\t--model_name_or_path \"gpt2\" \\\n",
        "\t--task_type \"causal_lm\" \\\n",
        "\t--train_file \"data/wikipedia-10.txt\" \\\n",
        "\t--max_seq_length 1024 \\\n",
        "\t--line_by_line \\\n",
        "\t--bias_type \"gender\" \\\n",
        "\t--cda_mode \"partial\" \\\n",
        "\t--output_dir \"checkpoints/gpt2-adapter-rf48\" \\\n",
        "\t--do_train \\\n",
        "\t--per_device_train_batch_size 4 \\\n",
        "\t--gradient_accumulation_steps 2 \\\n",
        "\t--learning_rate 5e-4 \\\n",
        "\t--num_train_epochs 2 \\\n",
        "\t--save_strategy \"epoch\" \\\n",
        "\t--evaluation_strategy \"epoch\" \\\n",
        "\t--seed 42 \\\n",
        "\t--down_sample 0.15 \\\n",
        "\t--adapter_config \"pfeiffer\" \\\n",
        "\t--adapter_reduction_factor 48"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ietxH-SmuXop",
        "outputId": "a54cc727-7ba1-4203-d5e7-0cac75895ead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-07 21:08:50.092727: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-07 21:08:50.092787: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-07 21:08:50.092820: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-07 21:08:50.100158: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-07 21:08:51.523130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "12/07/2023 21:08:54 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/07/2023 21:08:54 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=2,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/gpt2-adapter-rf48/runs/Dec07_21-08-54_42c692ccdb90,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=2.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=checkpoints/gpt2-adapter-rf48,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=checkpoints/gpt2-adapter-rf48,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.EPOCH,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "[INFO|configuration_utils.py:648] 2023-12-07 21:08:54,995 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-07 21:08:54,997 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2023-12-07 21:08:55,287 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:648] 2023-12-07 21:08:55,527 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-07 21:08:55,528 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2023-12-07 21:08:56,933 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:648] 2023-12-07 21:08:57,139 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:684] 2023-12-07 21:08:57,140 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1431] 2023-12-07 21:08:57,423 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1702] 2023-12-07 21:08:59,167 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1710] 2023-12-07 21:08:59,167 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|configuration.py:617] 2023-12-07 21:08:59,200 >> Adding adapter 'causal_lm'.\n",
            "tunable_param is 304320, frozen_param is 124439808\n",
            "Using saved cda dataset from: data/wikipedia-10-gender-1024-linebyline\n",
            "sampled train_dataset size: 351076 (15.0% of the augmented dataset)\n",
            "[INFO|trainer.py:229] 2023-12-07 21:09:30,618 >> The following columns in the training set  don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: special_tokens_mask.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1254] 2023-12-07 21:09:31,035 >> ***** Running training *****\n",
            "[INFO|trainer.py:1255] 2023-12-07 21:09:31,035 >>   Num examples = 351076\n",
            "[INFO|trainer.py:1256] 2023-12-07 21:09:31,035 >>   Num Epochs = 2\n",
            "[INFO|trainer.py:1257] 2023-12-07 21:09:31,035 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1258] 2023-12-07 21:09:31,035 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1259] 2023-12-07 21:09:31,035 >>   Gradient Accumulation steps = 2\n",
            "[INFO|trainer.py:1260] 2023-12-07 21:09:31,035 >>   Total optimization steps = 87768\n",
            "{'loss': 3.7237, 'learning_rate': 0.0004971515814419835, 'epoch': 0.01}\n",
            "{'loss': 3.6871, 'learning_rate': 0.0004943031628839668, 'epoch': 0.02}\n",
            "{'loss': 3.6537, 'learning_rate': 0.0004914547443259503, 'epoch': 0.03}\n",
            "{'loss': 3.6374, 'learning_rate': 0.0004886063257679337, 'epoch': 0.05}\n",
            "{'loss': 3.6328, 'learning_rate': 0.00048575790720991707, 'epoch': 0.06}\n",
            "{'loss': 3.6249, 'learning_rate': 0.00048290948865190047, 'epoch': 0.07}\n",
            "{'loss': 3.641, 'learning_rate': 0.0004800610700938839, 'epoch': 0.08}\n",
            "{'loss': 3.6568, 'learning_rate': 0.0004772126515358673, 'epoch': 0.09}\n",
            "{'loss': 3.6326, 'learning_rate': 0.0004743642329778507, 'epoch': 0.1}\n",
            "{'loss': 3.6207, 'learning_rate': 0.0004715158144198341, 'epoch': 0.11}\n",
            "{'loss': 3.642, 'learning_rate': 0.0004686673958618175, 'epoch': 0.13}\n",
            "{'loss': 3.6412, 'learning_rate': 0.000465818977303801, 'epoch': 0.14}\n",
            "{'loss': 3.6161, 'learning_rate': 0.0004629705587457844, 'epoch': 0.15}\n",
            "{'loss': 3.6264, 'learning_rate': 0.0004601221401877678, 'epoch': 0.16}\n",
            "{'loss': 3.6095, 'learning_rate': 0.0004572737216297512, 'epoch': 0.17}\n",
            "{'loss': 3.609, 'learning_rate': 0.0004544253030717346, 'epoch': 0.18}\n",
            "{'loss': 3.6102, 'learning_rate': 0.000451576884513718, 'epoch': 0.19}\n",
            "{'loss': 3.6033, 'learning_rate': 0.0004487284659557014, 'epoch': 0.21}\n",
            "{'loss': 3.6049, 'learning_rate': 0.00044588004739768485, 'epoch': 0.22}\n",
            "{'loss': 3.623, 'learning_rate': 0.00044303162883966825, 'epoch': 0.23}\n",
            "{'loss': 3.6141, 'learning_rate': 0.00044018321028165165, 'epoch': 0.24}\n",
            "{'loss': 3.6058, 'learning_rate': 0.00043733479172363505, 'epoch': 0.25}\n",
            "{'loss': 3.6212, 'learning_rate': 0.00043448637316561846, 'epoch': 0.26}\n",
            "{'loss': 3.6088, 'learning_rate': 0.00043163795460760186, 'epoch': 0.27}\n",
            "{'loss': 3.6066, 'learning_rate': 0.00042878953604958526, 'epoch': 0.28}\n",
            " 15% 12945/87768 [1:08:34<5:39:27,  3.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "@inproceedings{xie-lukasiewicz-2023-empirical,\n",
        "    title = \"An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models\",\n",
        "    author = \"Xie, Zhongbin  and\n",
        "      Lukasiewicz, Thomas\",\n",
        "    booktitle = \"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n",
        "    month = jul,\n",
        "    year = \"2023\",\n",
        "    address = \"Toronto, Canada\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://aclanthology.org/2023.acl-long.876\",\n",
        "    pages = \"15730--15745\",\n",
        "}"
      ],
      "metadata": {
        "id": "Ue-svgNfgGvW"
      }
    }
  ]
}